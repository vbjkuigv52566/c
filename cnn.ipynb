{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "label_dict = {\"cats\": 0, \"dogs\": 1}\n",
    "class_dict = {0: \"cats\", 1: \"dogs\"}\n",
    "N = 2\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, _data_dir, _transform, _loader):\n",
    "        self.labels = [_label for _label in os.listdir(_data_dir)]\n",
    "        _file_path_label_list = [(os.path.join(_data_dir, _label, _img_fn), _label)\n",
    "                                 for _label in os.listdir(_data_dir)\n",
    "                                 for _img_fn in os.listdir(os.path.join(_data_dir, _label))\n",
    "                                 if not os.path.isdir(os.path.join(_data_dir, _label, _img_fn))]\n",
    "\n",
    "        self.data = [(_loader(_fp), label_dict[_label]) for _fp, _label in _file_path_label_list]\n",
    "        self.transform = _transform\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        _img, _label = self.data[item]\n",
    "        _img = self.transform(_img)\n",
    "        return _img, _label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    print('data processing...')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.3),\n",
    "        transforms.RandomVerticalFlip(p=0.3),\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # normalization\n",
    "    ])\n",
    "    data_dir = \"data/training_data/\"\n",
    "    train_dataset = MyDataset(data_dir, transform, _loader=lambda _path: Image.open(_path).convert('RGB'))\n",
    "    test_dataset = MyDataset(data_dir, transform, _loader=lambda _path: Image.open(_path).convert('RGB'))\n",
    "\n",
    "    train_size = int(len(train_dataset) * 0.8)\n",
    "    validate_size = len(train_dataset) - train_size\n",
    "    train, val = torch.utils.data.random_split(train_dataset, [train_size, validate_size])\n",
    "\n",
    "    train_data_loader = DataLoader(dataset=train, batch_size=50, shuffle=True, num_workers=0)\n",
    "    val_data_loader = DataLoader(dataset=val, batch_size=50, shuffle=True, num_workers=0)\n",
    "    test_data_loader = DataLoader(dataset=test_dataset, batch_size=50, shuffle=False, num_workers=0)\n",
    "\n",
    "    return train_data_loader, val_data_loader, test_data_loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from classify.data_process import load_data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from classify.decorator import metric_time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "setup_seed(20)\n",
    "\n",
    "class cnn(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(cnn, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "            ),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "            ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "            ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.fc1 = nn.Linear(3 * 3 * 64, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.out = nn.Linear(10, N)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.softmax(self.out(x))\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_val_loss(model, Val):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    val_loss = []\n",
    "    for (data, target) in Val:\n",
    "        data, target = data.to(device), target.long().to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        val_loss.append(loss.cpu().item())\n",
    "\n",
    "    return np.mean(val_loss)\n",
    "\n",
    "\n",
    "@metric_time\n",
    "def train():\n",
    "    writer = SummaryWriter(\"log/\")\n",
    "    train_data_loader, val_data_loader, _ = load_data()\n",
    "    print('train...')\n",
    "    epoch_num = 30\n",
    "    best_model = None\n",
    "    min_epochs = 5\n",
    "    min_val_loss = 5\n",
    "    model = cnn().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0008)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    for epoch in tqdm(range(epoch_num), ascii=True):\n",
    "        train_loss = []\n",
    "        for batch_idx, (data, target) in enumerate(train_data_loader):\n",
    "            data, target = data.to(device), target.long().to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.cpu().item())\n",
    "        # validation\n",
    "        val_loss = get_val_loss(model, val_data_loader)\n",
    "        writer.add_scalar(\"val_loss\", val_loss, epoch)\n",
    "        if epoch + 1 > min_epochs and val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        tqdm.write('Epoch {:03d} train_loss {:.5f} val_loss {:.5f}'.format(epoch, np.mean(train_loss), val_loss))\n",
    "\n",
    "    torch.save(best_model.state_dict(), \"model/cnn.pkl\")\n",
    "\n",
    "\n",
    "@metric_time\n",
    "def test():\n",
    "    _, _, test_dataset = load_data()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = cnn().to(device)\n",
    "    model.load_state_dict(torch.load(\"model/cnn.pkl\"), False)\n",
    "    total = 0\n",
    "    current = 0\n",
    "    model.eval()\n",
    "    for (data, target) in test_dataset:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        outputs = model(data)\n",
    "        predicted = torch.max(outputs.data, 1)[1].data\n",
    "        total += target.size(0)\n",
    "        current += (predicted == target).sum()\n",
    "\n",
    "    print('Accuracy:%d%%' % (100 * current / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data processing...\n",
      "train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/30 [00:00<?, ?it/s]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "  3%|##7                                                                                | 1/30 [00:05<02:46,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 train_loss 0.69041 val_loss 0.71283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|#####5                                                                             | 2/30 [00:11<02:35,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 train_loss 0.66148 val_loss 0.73221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|########3                                                                          | 3/30 [00:16<02:27,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 train_loss 0.63053 val_loss 0.74118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|###########                                                                        | 4/30 [00:21<02:21,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 train_loss 0.60586 val_loss 0.59955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|#############8                                                                     | 5/30 [00:27<02:16,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 train_loss 0.58516 val_loss 0.59193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|################6                                                                  | 6/30 [00:32<02:11,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 train_loss 0.56396 val_loss 0.61281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|###################3                                                               | 7/30 [00:38<02:05,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006 train_loss 0.53831 val_loss 0.56035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|######################1                                                            | 8/30 [00:43<01:59,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 train_loss 0.53220 val_loss 0.58576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|########################9                                                          | 9/30 [00:49<01:54,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008 train_loss 0.51465 val_loss 0.59177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|###########################3                                                      | 10/30 [00:54<01:49,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009 train_loss 0.51262 val_loss 0.56475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|##############################                                                    | 11/30 [01:00<01:44,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 train_loss 0.50500 val_loss 0.64652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|################################8                                                 | 12/30 [01:05<01:39,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 011 train_loss 0.49751 val_loss 0.58299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|###################################5                                              | 13/30 [01:11<01:34,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012 train_loss 0.49327 val_loss 0.59778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|######################################2                                           | 14/30 [01:17<01:28,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 013 train_loss 0.47771 val_loss 0.56692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|#########################################                                         | 15/30 [01:22<01:23,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014 train_loss 0.46802 val_loss 0.55517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|###########################################7                                      | 16/30 [01:28<01:18,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 015 train_loss 0.47614 val_loss 0.66633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|##############################################4                                   | 17/30 [01:33<01:12,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016 train_loss 0.45777 val_loss 0.63282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|#################################################1                                | 18/30 [01:39<01:07,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 017 train_loss 0.44926 val_loss 0.49766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|###################################################9                              | 19/30 [01:44<01:01,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 018 train_loss 0.43818 val_loss 0.60943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|######################################################6                           | 20/30 [01:50<00:55,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 019 train_loss 0.43715 val_loss 0.53149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|#########################################################4                        | 21/30 [01:56<00:49,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020 train_loss 0.42777 val_loss 0.54367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|############################################################1                     | 22/30 [02:01<00:44,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 021 train_loss 0.42167 val_loss 0.53923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|##############################################################8                   | 23/30 [02:07<00:39,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 022 train_loss 0.41583 val_loss 0.52610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|#################################################################6                | 24/30 [02:12<00:33,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 023 train_loss 0.41593 val_loss 0.53730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|####################################################################3             | 25/30 [02:18<00:27,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 024 train_loss 0.39452 val_loss 0.57724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|#######################################################################           | 26/30 [02:23<00:22,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 025 train_loss 0.40114 val_loss 0.55945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|#########################################################################8        | 27/30 [02:29<00:16,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 026 train_loss 0.40143 val_loss 0.56093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|############################################################################5     | 28/30 [02:35<00:11,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 027 train_loss 0.39803 val_loss 0.56150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|###############################################################################2  | 29/30 [02:40<00:05,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 028 train_loss 0.40357 val_loss 0.56026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##################################################################################| 30/30 [02:46<00:00,  5.54s/it]\n",
      "\u001b[32m2024-11-20 11:11:46.038\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mclassify.decorator\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mtrain运行时间: 173.88456869125366 s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 029 train_loss 0.39236 val_loss 0.52163\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.activation.Softmax"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_12848/1345067293.py:140: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model/cnn.pkl\"), False)\n",
      "\u001b[32m2024-11-20 11:11:52.981\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mclassify.decorator\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mtest运行时间: 6.919210910797119 s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:85%\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple\n",
      "Requirement already satisfied: tensorboard in c:\\programdata\\anaconda3\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: loguru in c:\\programdata\\anaconda3\\lib\\site-packages (0.7.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (58.0.4)\n",
      "Requirement already satisfied: six>1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (21.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (1.20.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (5.28.3)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (2.0.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (1.68.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: colorama>=0.3.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from loguru) (0.4.4)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from loguru) (1.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->tensorboard) (3.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorboard loguru -i https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试猫狗二分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_2596/3000547427.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model/cnn.pkl\"), False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0073, -0.4544]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "from classify.cnn import cnn\n",
    "from classify.data_process import class_dict\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = cnn().to(device)\n",
    "model.load_state_dict(torch.load(\"model/cnn.pkl\"), False)\n",
    "model.eval()\n",
    "\n",
    "_img_path = \"data/testing_data/dogs/dog.1031.jpg\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # normalization\n",
    "])\n",
    "img = Image.open(_img_path).convert('RGB')\n",
    "# 模拟批样本\n",
    "img_transform = transform(img).unsqueeze(0)\n",
    "\n",
    "output = model(img_transform)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs\n"
     ]
    }
   ],
   "source": [
    "pred = class_dict[torch.max(output.data, 1)[1].data.item()]\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir_spec=jupyterlab:c:\\\\Users\\\\admin\\\\desktop\\\\dog_cat_classify\\\\dog_cat_classify\\\\log --port 8000 --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
